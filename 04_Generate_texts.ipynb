{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#os.environ[\"NCCL_P2P_DISABLE\"]=\"1\"\n",
    "prompt_template = \"\"\"You have been assigned a retrieval task about {topic}. \n",
    "With the following queries: \n",
    "{questions}\n",
    "\n",
    "Produce two german document, each at least 100 words long, on the subject of {topic}. These documents should be composed in a style that mirrors the type of content one would typically find when searching for answers to a question, such as a Wikipedia article, blog post, news article, list, advertisement etc. Never create documents that only advice on how or where to search for information! For example, if the query is \"Search for information about the history of Berlin\", the document should provide a detailed account of Berlin's history, rather than general advice on how to search for historical information. The style of the documents should mimic the type of results that the question is searching for. Both texts should be of similar length to ensure consistency when comparing them.\n",
    "\n",
    "The first document serves as a 'hard negative' example. It should discuss close to the topic of {topic}, but it should never answer the queries!:\n",
    "{questions}\n",
    "Again the hard negative should never provide the answer to the query. For instance, if the query is \"When is Costco open?\", the hard negative example might discuss the opening hours of Walmart instead.\n",
    "\n",
    "The second document should act as a 'positive' example. It should directly answer the queries:\n",
    "{questions}\n",
    "This document should be informative and precise, offering a specific answer or solution to the queries. Always create both documents in german!\"\"\"# prompt_template = \"\"\"You have been assigned a retrieval task {topic}\n",
    "# With the following queries: \n",
    "# {questions}\n",
    "\n",
    "# Your mission is to write one text retrieval example for this task with the following elements:\n",
    "# - \"positive_document\": a relevant document for the query.\n",
    "# - \"hard_negative_document\": a hard negative document that only appears relevant to the query.\n",
    "\n",
    "# Please adhere to the following guidelines:\n",
    "# - All documents must be created independent of the query. Avoid copying the query verbatim. It’s acceptable if some parts of the \"positive_document\" are not topically related to the query.\n",
    "# - All documents should be at least 100 words long.\n",
    "# - The \"hard_negative_document\" contains some useful information, but it should be less useful or comprehensive compared to the \"positive_document\".\n",
    "# - The documents should be in german.\n",
    "# - Do not provide any explanation in any document on why it is relevant or not relevant to the query.\n",
    "\n",
    "# - Both the query and documents require college level education to understand.\"\"\"\n",
    "\n",
    "\n",
    "response_template = \"\"\"Hard negative german document (not containing the viable information for the queries!):\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-05 16:21:33 config.py:552] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 02-05 16:21:33 config.py:177] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 16:21:36,682\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of required GPUs exceeds the total number of available GPUs in the cluster.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mvllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgptq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgptq-4bit-32g-actorder_True\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m               \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# disable_custom_all_reduce=True\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm/vllm/entrypoints/llm.py:109\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     91\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m     92\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     93\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    108\u001b[0m )\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/vllm/vllm/engine/llm_engine.py:351\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args)\u001b[0m\n\u001b[1;32m    349\u001b[0m parallel_config \u001b[38;5;241m=\u001b[39m engine_configs[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# Initialize the cluster.\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m placement_group \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39mengine_configs,\n\u001b[1;32m    354\u001b[0m              placement_group,\n\u001b[1;32m    355\u001b[0m              log_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m engine_args\u001b[38;5;241m.\u001b[39mdisable_log_stats)\n",
      "File \u001b[0;32m~/vllm/vllm/engine/ray_utils.py:111\u001b[0m, in \u001b[0;36minitialize_cluster\u001b[0;34m(parallel_config, engine_use_ray, ray_address)\u001b[0m\n\u001b[1;32m    109\u001b[0m num_gpus_in_cluster \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mcluster_resources()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel_config\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m num_gpus_in_cluster:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of required GPUs exceeds the total number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavailable GPUs in the cluster.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Create a new placement group\u001b[39;00m\n\u001b[1;32m    115\u001b[0m placement_group_specs \u001b[38;5;241m=\u001b[39m ([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}] \u001b[38;5;241m*\u001b[39m parallel_config\u001b[38;5;241m.\u001b[39mworld_size)\n",
      "\u001b[0;31mValueError\u001b[0m: The number of required GPUs exceeds the total number of available GPUs in the cluster."
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import vllm \n",
    "import pandas as pd \n",
    "from vllm import SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "sampling_params = SamplingParams(temperature=0.1, max_tokens=16000)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "llm = vllm.LLM(model=model_name, quantization=\"gptq\", dtype=torch.float16, tensor_parallel_size=2, max_model_len=16000, revision=\"gptq-4bit-32g-actorder_True\", gpu_memory_utilization=0.75, \n",
    "               enforce_eager=True, \n",
    "               # disable_custom_all_reduce=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "df = pd.read_parquet(\"03_parsed_questions.parquet\")\n",
    "df[[\"Positive\", \"Hard Negative\"]] = np.nan\n",
    "df = df.iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1769 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def generate_prompt(row):\n",
    "    row = row.fillna(\"\")\n",
    "    questions = \"\\n\".join(row[[\"Imperative Form\", \"Question\", \"Search String\"]].str.removesuffix('\"').str.removeprefix('\"').to_list())\n",
    "    topic = row[\"topic\"]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(conversation=[\n",
    "        {\"role\": \"user\", \"content\":prompt_template.replace(\"{questions}\", str(questions)).replace(\"{topic}\", str(topic))},\n",
    "        {\"role\": \"assistant\", \"content\":response_template}\n",
    "        ], tokenize=False)\n",
    "    formatted_prompt = formatted_prompt.removesuffix(\"</s>\")\n",
    "    return formatted_prompt\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "df = pd.read_parquet(\"04_results_texts_v5.parquet\")\n",
    "df_nan = df[df[\"raw_texts\"]==\"nan\"]\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(df_nan), BATCH_SIZE)):\n",
    "    batches = df_nan[[\"topic\", \"Imperative Form\", \"Question\", \"Search String\"]].iloc[i:i+BATCH_SIZE]\n",
    "    formatted_prompt =[generate_prompt(batch) for n, batch in batches.iterrows()]\n",
    "    results = llm.generate(formatted_prompt, sampling_params=sampling_params)\n",
    "    results_adj = [result.prompt.split(\"[/INST]\")[-1]+ result.outputs[0].text for result in results]\n",
    "    df.loc[batches.index, 'raw_texts'] = results_adj\n",
    "    df.to_parquet(\"04_results_texts_v5.parquet\")   \n",
    "\n",
    "\n",
    "# vllm 0.2.7\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:03<00:00,  3.87s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:15<00:00,  4.25s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:14<00:00,  4.21s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:17<00:00,  4.31s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:07<00:00,  3.98s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:16<00:00,  4.28s/it]\n",
    "\n",
    "# vllm 0.3: disable cuda graph & all reduce\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:13<00:00,  4.17s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:39<00:00,  4.99s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:05<00:00,  3.93s/it]\n",
    "    \n",
    "# vllm 0.3: disable all reduce\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:14<00:00,  4.22s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:07<00:00,  3.99s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:03<00:00,  3.86s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:00<00:00,  3.78s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:01<00:00,  3.80s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [01:56<00:00,  3.65s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:08<00:00,  4.01s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:13<00:00,  4.16s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:08<00:00,  4.03s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [01:53<00:00,  3.54s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [01:45<00:00,  3.28s/it]\n",
    "# Processed prompts: 100%|██████████| 32/32 [02:00<00:00,  3.76s/it]\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnovi_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
