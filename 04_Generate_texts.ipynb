{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Generate two texts related to the topic of {topic}, both written in German and each being at least 100 words long. The texts should be generated in a style typical to answer the question. Never create generic texts explaining how or where to search. \n",
    "\n",
    "The first text should be a hard negative example. It should be related to the question or search string about {topic}, but it shouldn't answer the question:\n",
    "{questions}\n",
    "This text should talk about the topic in a similar way but avoid giving the answer under any circumstances. For instance, if the question is \"When is Costco open?\", the hard negative example might discuss Walmart's opening hours instead. Remember, the hard negative example should never give the answer to the questions.\n",
    "\n",
    "The second text should be a positive example. It must provide the solution to the question:\n",
    "{questions}\n",
    "This text should be an accurate and informative piece that fully explores the topic and answers the questions. Craft a response that directly tackles the underlying question by providing a specific answer, search result, or solution, rather than giving broad advice or unrelated information. \n",
    "For example, if the question is \"Search for information about the history of Berlin\", provide a detailed account of Berlin's history, rather than general advice on how or where to search for historical information. Mimic the style of results the question searches for! Both texts should be of similar length to ensure consistency in comparison and should be written in German.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# prompt_template = \"\"\"You have been assigned a retrieval task {topic}\n",
    "# With the following queries: \n",
    "# {questions}\n",
    "\n",
    "# Your mission is to write one text retrieval example for this task with the following elements:\n",
    "# - \"positive_document\": a relevant document for the query.\n",
    "# - \"hard_negative_document\": a hard negative document that only appears relevant to the query.\n",
    "\n",
    "# Please adhere to the following guidelines:\n",
    "# - All documents must be created independent of the query. Avoid copying the query verbatim. Itâ€™s acceptable if some parts of the \"positive_document\" are not topically related to the query.\n",
    "# - All documents should be at least 100 words long.\n",
    "# - The \"hard_negative_document\" contains some useful information, but it should be less useful or comprehensive compared to the \"positive_document\".\n",
    "# - The documents should be in german.\n",
    "# - Do not provide any explanation in any document on why it is relevant or not relevant to the query.\n",
    "\n",
    "# - Both the query and documents require college level education to understand.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "response_template = \"\"\"Hard negative example (not containing the answer to the questions!):\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-30 14:24:29 config.py:506] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 01-30 14:24:29 config.py:176] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 14:24:35,555\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-30 14:24:36 llm_engine.py:72] Initializing an LLM engine with config: model='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer_mode=auto, revision=gptq-4bit-32g-actorder_True, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=16000, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=gptq, enforce_eager=False, seed=0)\n",
      "INFO 01-30 14:24:41 weight_utils.py:164] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerVllm pid=1715487)\u001b[0m INFO 01-30 14:24:42 weight_utils.py:164] Using model weights format ['*.safetensors']\n",
      "INFO 01-30 14:24:54 llm_engine.py:316] # GPU blocks: 1955, # CPU blocks: 4096\n",
      "INFO 01-30 14:24:54 model_runner.py:625] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-30 14:24:54 model_runner.py:629] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=1715487)\u001b[0m INFO 01-30 14:24:54 model_runner.py:625] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=1715487)\u001b[0m INFO 01-30 14:24:54 model_runner.py:629] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n",
      "\u001b[36m(RayWorkerVllm pid=1715487)\u001b[0m [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=1715487)\u001b[0m INFO 01-30 14:25:29 model_runner.py:689] Graph capturing finished in 35 secs.\n",
      "INFO 01-30 14:25:29 model_runner.py:689] Graph capturing finished in 35 secs.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import vllm \n",
    "import pandas as pd \n",
    "from vllm import SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "sampling_params = SamplingParams(temperature=0.1, max_tokens=16000)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "llm = vllm.LLM(model=model_name, quantization=\"gptq\", dtype=torch.float16, tensor_parallel_size=2, max_model_len=16000, revision=\"gptq-4bit-32g-actorder_True\", gpu_memory_utilization=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "df = pd.read_parquet(\"03_parsed_questions.parquet\")\n",
    "df[[\"Positive\", \"Hard Negative\"]] = np.nan\n",
    "df = df.iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2583 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def generate_prompt(row):\n",
    "    row = row.fillna(\"\")\n",
    "    questions = \"\\n\".join(row[[\"Imperative Form\", \"Question\", \"Search String\"]].str.removesuffix('\"').str.removeprefix('\"').to_list())\n",
    "    topic = row[\"topic\"]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(conversation=[\n",
    "        {\"role\": \"user\", \"content\":prompt_template.replace(\"{questions}\", str(questions)).replace(\"{topic}\", str(topic))},\n",
    "        {\"role\": \"assistant\", \"content\":response_template}\n",
    "        ], tokenize=False)\n",
    "    formatted_prompt = formatted_prompt.removesuffix(\"</s>\")\n",
    "    return formatted_prompt\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "df = pd.read_parquet(\"04_results_texts_v3.parquet\")\n",
    "df_nan = df[df[\"raw_texts\"]==\"nan\"]\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(df_nan), BATCH_SIZE)):\n",
    "    batches = df_nan[[\"topic\", \"Imperative Form\", \"Question\", \"Search String\"]].iloc[i:i+BATCH_SIZE]\n",
    "    formatted_prompt =[generate_prompt(batch) for n, batch in batches.iterrows()]\n",
    "    results = llm.generate(formatted_prompt, sampling_params=sampling_params)\n",
    "    results_adj = [result.prompt.split(\"[/INST]\")[-1]+ result.outputs[0].text for result in results]\n",
    "    df.loc[batches.index, 'raw_texts'] = results_adj\n",
    "    df.to_parquet(\"04_results_texts_v3.parquet\")   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnovi_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
