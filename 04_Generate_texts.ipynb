{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"NCCL_P2P_DISABLE\"]=\"1\"\n",
    "prompt_template = \"\"\"You have been assigned a retrieval task about {topic}. \n",
    "With the following queries: \n",
    "{questions}\n",
    "\n",
    "Produce two german document, each at least 100 words long, on the subject of {topic}. These documents should be composed in a style that mirrors the type of content one would typically find when searching for answers to a question, such as a Wikipedia article, blog post, news article, list, advertisement etc. Never create documents that only advice on how or where to search for information! For example, if the query is \"Search for information about the history of Berlin\", the document should provide a detailed account of Berlin's history, rather than general advice on how to search for historical information. The style of the documents should mimic the type of results that the question is searching for. Both texts should be of similar length to ensure consistency when comparing them.\n",
    "\n",
    "The first document serves as a 'hard negative' example. It should discuss close to the topic of {topic}, but it should never answer the queries!:\n",
    "{questions}\n",
    "Again the hard negative should never provide the answer to the query. For instance, if the query is \"When is Costco open?\", the hard negative example might discuss the opening hours of Walmart instead.\n",
    "\n",
    "The second document should act as a 'positive' example. It should directly answer the queries:\n",
    "{questions}\n",
    "This document should be informative and precise, offering a specific answer or solution to the queries. Always create both documents in german!\"\"\"# prompt_template = \"\"\"You have been assigned a retrieval task {topic}\n",
    "# With the following queries: \n",
    "# {questions}\n",
    "\n",
    "# Your mission is to write one text retrieval example for this task with the following elements:\n",
    "# - \"positive_document\": a relevant document for the query.\n",
    "# - \"hard_negative_document\": a hard negative document that only appears relevant to the query.\n",
    "\n",
    "# Please adhere to the following guidelines:\n",
    "# - All documents must be created independent of the query. Avoid copying the query verbatim. It’s acceptable if some parts of the \"positive_document\" are not topically related to the query.\n",
    "# - All documents should be at least 100 words long.\n",
    "# - The \"hard_negative_document\" contains some useful information, but it should be less useful or comprehensive compared to the \"positive_document\".\n",
    "# - The documents should be in german.\n",
    "# - Do not provide any explanation in any document on why it is relevant or not relevant to the query.\n",
    "\n",
    "# - Both the query and documents require college level education to understand.\"\"\"\n",
    "\n",
    "\n",
    "response_template = \"\"\"Hard negative german document (not containing the viable information for the queries!):\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-02 11:23:03 config.py:457] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 02-02 11:23:03 config.py:175] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 11:23:06,397\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-02 11:23:06 llm_engine.py:70] Initializing an LLM engine with config: model='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer_mode=auto, revision=gptq-4bit-32g-actorder_True, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=16000, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=gptq, enforce_eager=True, seed=0)\n",
      "INFO 02-02 11:23:21 llm_engine.py:275] # GPU blocks: 1636, # CPU blocks: 4096\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import vllm \n",
    "import pandas as pd \n",
    "from vllm import SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "sampling_params = SamplingParams(temperature=0.1, max_tokens=16000)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "llm = vllm.LLM(model=model_name, quantization=\"gptq\", dtype=torch.float16, tensor_parallel_size=2, max_model_len=16000, revision=\"gptq-4bit-32g-actorder_True\", gpu_memory_utilization=0.75, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "df = pd.read_parquet(\"03_parsed_questions.parquet\")\n",
    "df[[\"Positive\", \"Hard Negative\"]] = np.nan\n",
    "df = df.iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [02:03<00:00,  3.87s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:15<00:00,  4.25s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:14<00:00,  4.21s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:17<00:00,  4.31s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:07<00:00,  3.98s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:16<00:00,  4.28s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:14<00:00,  4.20s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:31<00:00,  4.72s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:15<00:00,  4.25s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:19<00:00,  4.36s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:14<00:00,  4.22s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:20<00:00,  4.38s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:29<00:00,  4.66s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:27<00:00,  4.60s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:11<00:00,  4.09s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:05<00:00,  3.91s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:10<00:00,  4.07s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [01:43<00:00,  3.25s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:27<00:00,  4.61s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:24<00:00,  4.52s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:18<00:00,  4.33s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:06<00:00,  3.95s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:07<00:00,  3.99s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:14<00:00,  4.20s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:17<00:00,  4.28s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:27<00:00,  4.62s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:05<00:00,  3.92s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:15<00:00,  4.23s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [09:10<00:00, 17.21s/it] \n",
      "Processed prompts: 100%|██████████| 32/32 [02:13<00:00,  4.18s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:28<00:00,  4.65s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:06<00:00,  3.96s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:20<00:00,  4.38s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:22<00:00,  4.46s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:08<00:00,  4.02s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:18<00:00,  4.33s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:07<00:00,  3.97s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [08:51<00:00, 16.60s/it] \n",
      "Processed prompts: 100%|██████████| 32/32 [02:02<00:00,  3.84s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [01:59<00:00,  3.72s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [01:51<00:00,  3.50s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:13<00:00,  4.17s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:06<00:00,  3.94s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:25<00:00,  4.55s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:18<00:00,  4.33s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:01<00:00,  3.79s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:12<00:00,  4.15s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:17<00:00,  4.31s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:27<00:00,  4.62s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:12<00:00,  4.13s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:05<00:00,  3.92s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [01:53<00:00,  3.55s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:18<00:00,  4.34s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:27<00:00,  4.61s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [01:58<00:00,  3.72s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [08:53<00:00, 16.67s/it] \n",
      "Processed prompts: 100%|██████████| 32/32 [02:10<00:00,  4.06s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:20<00:00,  4.38s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:21<00:00,  4.43s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [01:54<00:00,  3.59s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:10<00:00,  4.07s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:20<00:00,  4.40s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:16<00:00,  4.28s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:21<00:00,  4.42s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:24<00:00,  4.52s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:39<00:00,  5.00s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:37<00:00,  4.91s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:07<00:00,  3.99s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:13<00:00,  4.17s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:33<00:00,  4.79s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:23<00:00,  4.47s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:19<00:00,  4.36s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:26<00:00,  4.59s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:22<00:00,  4.47s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:03<00:00,  3.86s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [01:58<00:00,  3.70s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:11<00:00,  4.10s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:06<00:00,  3.94s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:09<00:00,  4.03s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:21<00:00,  4.42s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:15<00:00,  4.23s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:06<00:00,  3.94s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:24<00:00,  4.50s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:18<00:00,  4.34s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [01:59<00:00,  3.72s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:07<00:00,  3.98s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:07<00:00,  3.98s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:11<00:00,  4.12s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:22<00:00,  4.45s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:18<00:00,  4.33s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:07<00:00,  4.00s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:09<00:00,  4.03s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:37<00:00,  4.91s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:18<00:00,  4.34s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:28<00:00,  4.65s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:30<00:00,  4.71s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:07<00:00,  3.99s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:22<00:00,  4.44s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:07<00:00,  3.97s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:15<00:00,  4.23s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:20<00:00,  4.38s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:25<00:00,  4.53s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:30<00:00,  4.72s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:26<00:00,  4.56s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:21<00:00,  4.43s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:08<00:00,  4.01s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:27<00:00,  4.60s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:14<00:00,  4.21s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:11<00:00,  4.12s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:18<00:00,  4.33s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:09<00:00,  4.03s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:13<00:00,  4.16s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:04<00:00,  3.90s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:42<00:00,  5.09s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:35<00:00,  4.85s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:45<00:00,  5.18s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:11<00:00,  4.10s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:24<00:00,  4.51s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:18<00:00,  4.32s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:26<00:00,  4.59s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:32<00:00,  4.78s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:22<00:00,  4.45s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:21<00:00,  4.42s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:14<00:00,  4.20s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:16<00:00,  4.27s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:13<00:00,  4.16s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:19<00:00,  4.35s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:18<00:00,  4.34s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:40<00:00,  5.03s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:26<00:00,  4.58s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:14<00:00,  4.20s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:10<00:00,  4.07s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:21<00:00,  4.41s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:17<00:00,  4.29s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:25<00:00,  4.54s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:31<00:00,  4.73s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:11<00:00,  4.12s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:27<00:00,  4.61s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [01:54<00:00,  3.59s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:09<00:00,  4.05s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:27<00:00,  4.61s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:33<00:00,  4.78s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [09:03<00:00, 16.99s/it] \n",
      "Processed prompts: 100%|██████████| 32/32 [02:00<00:00,  3.75s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:12<00:00,  4.15s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:19<00:00,  4.35s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:26<00:00,  4.57s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:30<00:00,  4.69s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:08<00:00,  4.03s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:30<00:00,  4.72s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:42<00:00,  5.07s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:27<00:00,  4.62s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:20<00:00,  4.38s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:19<00:00,  4.35s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:23<00:00,  4.50s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:22<00:00,  4.44s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:15<00:00,  4.24s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:08<00:00,  4.02s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [01:57<00:00,  3.67s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:05<00:00,  3.93s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:06<00:00,  3.96s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:12<00:00,  4.15s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:14<00:00,  4.21s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:33<00:00,  4.79s/it]\n",
      "  8%|▊         | 164/2016 [6:43:55<76:01:28, 147.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m batches \u001b[38;5;241m=\u001b[39m df_nan[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImperative Form\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch String\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39miloc[i:i\u001b[38;5;241m+\u001b[39mBATCH_SIZE]\n\u001b[1;32m     23\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m[generate_prompt(batch) \u001b[38;5;28;01mfor\u001b[39;00m n, batch \u001b[38;5;129;01min\u001b[39;00m batches\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[0;32m---> 24\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m results_adj \u001b[38;5;241m=\u001b[39m [result\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m result\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m     26\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[batches\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_texts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m results_adj\n",
      "File \u001b[0;32m~/miniforge3/envs/qnovi_app/lib/python3.11/site-packages/vllm/entrypoints/llm.py:165\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm)\u001b[0m\n\u001b[1;32m    162\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m prompt_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m prompt_token_ids[\n\u001b[1;32m    163\u001b[0m         i]\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_request(prompt, sampling_params, token_ids)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/qnovi_app/lib/python3.11/site-packages/vllm/entrypoints/llm.py:185\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    183\u001b[0m outputs: List[RequestOutput] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 185\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/miniforge3/envs/qnovi_app/lib/python3.11/site-packages/vllm/engine/llm_engine.py:628\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m seq_group_metadata_list, scheduler_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mschedule()\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# Execute the model.\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     all_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexecute_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq_group_metadata_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_swap_in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_swap_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_copy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# Only the driver worker returns the sampling results.\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     output \u001b[38;5;241m=\u001b[39m all_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/qnovi_app/lib/python3.11/site-packages/vllm/engine/llm_engine.py:795\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m     driver_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# Start the driver worker after all the ray workers.\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# Get the results of the ray workers.\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers:\n",
      "File \u001b[0;32m~/miniforge3/envs/qnovi_app/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/qnovi_app/lib/python3.11/site-packages/vllm/worker/worker.py:189\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_seq_groups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 189\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniforge3/envs/qnovi_app/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/qnovi_app/lib/python3.11/site-packages/vllm/worker/model_runner.py:446\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode()\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    442\u001b[0m     seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],\n\u001b[1;32m    443\u001b[0m     kv_caches: List[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    444\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[1;32m    445\u001b[0m     input_tokens, input_positions, input_metadata, sampling_metadata \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 446\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_input_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# Execute the model.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_metadata\u001b[38;5;241m.\u001b[39muse_cuda_graph:\n",
      "File \u001b[0;32m~/miniforge3/envs/qnovi_app/lib/python3.11/site-packages/vllm/worker/model_runner.py:340\u001b[0m, in \u001b[0;36mModelRunner.prepare_input_tensors\u001b[0;34m(self, seq_group_metadata_list)\u001b[0m\n\u001b[1;32m    336\u001b[0m     (input_tokens, input_positions, input_metadata,\n\u001b[1;32m    337\u001b[0m      prompt_lens) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_prompt(seq_group_metadata_list)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m     (input_tokens, input_positions, input_metadata\n\u001b[0;32m--> 340\u001b[0m      ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     prompt_lens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    342\u001b[0m sampling_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_sample(seq_group_metadata_list,\n\u001b[1;32m    343\u001b[0m                                          prompt_lens)\n",
      "File \u001b[0;32m~/miniforge3/envs/qnovi_app/lib/python3.11/site-packages/vllm/worker/model_runner.py:238\u001b[0m, in \u001b[0;36mModelRunner._prepare_decode\u001b[0;34m(self, seq_group_metadata_list)\u001b[0m\n\u001b[1;32m    236\u001b[0m     block_tables \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_block_tables, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     block_tables \u001b[38;5;241m=\u001b[39m \u001b[43m_make_tensor_with_pad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_context_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m input_metadata \u001b[38;5;241m=\u001b[39m InputMetadata(\n\u001b[1;32m    247\u001b[0m     is_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    248\u001b[0m     slot_mapping\u001b[38;5;241m=\u001b[39mslot_mapping,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     use_cuda_graph\u001b[38;5;241m=\u001b[39muse_captured_graph,\n\u001b[1;32m    253\u001b[0m )\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_tokens, input_positions, input_metadata\n",
      "File \u001b[0;32m~/miniforge3/envs/qnovi_app/lib/python3.11/site-packages/vllm/worker/model_runner.py:645\u001b[0m, in \u001b[0;36m_make_tensor_with_pad\u001b[0;34m(x, max_len, pad, dtype, device, pin_memory)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_tensor_with_pad\u001b[39m(\n\u001b[1;32m    637\u001b[0m     x: List[List[\u001b[38;5;28mint\u001b[39m]],\n\u001b[1;32m    638\u001b[0m     max_len: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     pin_memory: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    644\u001b[0m     padded_x \u001b[38;5;241m=\u001b[39m [_pad_to_max(x_i, max_len, pad) \u001b[38;5;28;01mfor\u001b[39;00m x_i \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def generate_prompt(row):\n",
    "    row = row.fillna(\"\")\n",
    "    questions = \"\\n\".join(row[[\"Imperative Form\", \"Question\", \"Search String\"]].str.removesuffix('\"').str.removeprefix('\"').to_list())\n",
    "    topic = row[\"topic\"]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(conversation=[\n",
    "        {\"role\": \"user\", \"content\":prompt_template.replace(\"{questions}\", str(questions)).replace(\"{topic}\", str(topic))},\n",
    "        {\"role\": \"assistant\", \"content\":response_template}\n",
    "        ], tokenize=False)\n",
    "    formatted_prompt = formatted_prompt.removesuffix(\"</s>\")\n",
    "    return formatted_prompt\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "df = pd.read_parquet(\"04_results_texts_v5.parquet\")\n",
    "df_nan = df[df[\"raw_texts\"]==\"nan\"]\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(df_nan), BATCH_SIZE)):\n",
    "    batches = df_nan[[\"topic\", \"Imperative Form\", \"Question\", \"Search String\"]].iloc[i:i+BATCH_SIZE]\n",
    "    formatted_prompt =[generate_prompt(batch) for n, batch in batches.iterrows()]\n",
    "    results = llm.generate(formatted_prompt, sampling_params=sampling_params)\n",
    "    results_adj = [result.prompt.split(\"[/INST]\")[-1]+ result.outputs[0].text for result in results]\n",
    "    df.loc[batches.index, 'raw_texts'] = results_adj\n",
    "    df.to_parquet(\"04_results_texts_v5.parquet\")   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnovi_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
