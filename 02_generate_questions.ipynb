{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fragen erzeugen als search string, Question und imperativ question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_response = \"\"\"\"Bestimme das Jahr, in dem das Apple File System (APFS) von Apple vorgestellt wurde.\"\n",
    "Question: \"In welchem Jahr wurde das Apple File System (APFS) von Apple vorgestellt?\"\n",
    "Search String: \"Jahr Apple File System (APFS) vorgestellt\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a Data Scientist wanting to generate a Dataset for Training an Embedding model for retreival tasks. Therefore in the first step generate an instruction, question or search string with the topic:\n",
    "{TOPICCC}\n",
    "Generate the questions/searchstring/instruction in the following form:\n",
    "Imperative Form: [This is like telling someone to do something. Command or order. It's a sentence that gives direct advice or instruction on the topic. Do not use a \"!\" at the end]\n",
    "Question: [This is like asking someone about something.]\n",
    "Search String: [A set of words that you would type into a search engine to find information on the internet.]\n",
    "\n",
    "All of the generated question/instruction/searchstring should only be different formulations of each other and be about the same question and in german. Do not use \"Sie\"\n",
    "\"\"\"\n",
    "\n",
    "response_template =\"{TOPICCC}\\nImperative Form: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "df = pd.read_parquet(\"results_parsed.parquet\")\n",
    "df = df.explode(column=\"questions\")\n",
    "df.reset_index(inplace=True)\n",
    "df[\"gen_questions\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-25 10:05:24 config.py:457] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 01-25 10:05:24 config.py:175] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 10:05:26,854\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-25 10:05:27 llm_engine.py:70] Initializing an LLM engine with config: model='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer_mode=auto, revision=gptq-4bit-32g-actorder_True, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2000, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=gptq, enforce_eager=False, seed=0)\n",
      "INFO 01-25 10:05:36 llm_engine.py:275] # GPU blocks: 3497, # CPU blocks: 4096\n",
      "INFO 01-25 10:05:36 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-25 10:05:36 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "\u001b[36m(RayWorkerVllm pid=484370)\u001b[0m INFO 01-25 10:05:36 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=484370)\u001b[0m INFO 01-25 10:05:36 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=484370)\u001b[0m [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n",
      "[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-25 10:06:11 model_runner.py:547] Graph capturing finished in 35 secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=484370)\u001b[0m INFO 01-25 10:06:11 model_runner.py:547] Graph capturing finished in 35 secs.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import vllm \n",
    "import pandas as pd \n",
    "from vllm import SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "sampling_params = SamplingParams(temperature=0.1, max_tokens=4000)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "llm = vllm.LLM(model=model_name, quantization=\"gptq\", dtype=torch.float16, tensor_parallel_size=2, max_model_len=2000, revision=\"gptq-4bit-32g-actorder_True\", gpu_memory_utilization=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [01:09<00:00,  2.16s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:49<00:00,  1.56s/it]\n",
      "Processed prompts:  62%|██████▎   | 20/32 [13:02<07:49, 39.11s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:56<00:00,  1.76s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:49<00:00,  1.53s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:52<00:00,  1.64s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:50<00:00,  1.57s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:49<00:00,  1.56s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:45<00:00,  1.42s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:55<00:00,  1.73s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:50<00:00,  1.58s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:40<00:00,  1.27s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:40<00:00,  1.25s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:59<00:00,  1.85s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:39<00:00,  1.24s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:40<00:00,  1.26s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:47<00:00,  1.47s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [00:51<00:00,  1.61s/it]\n",
      "  1%|          | 22/2583 [17:41<33:00:02, 46.39s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def generate_prompt(questions):\n",
    "    formatted_prompt = tokenizer.apply_chat_template(conversation=[\n",
    "        {\"role\": \"user\", \"content\": prompt.replace(\"{TOPICCC}\", \"Apple File System (APFS)\")},\n",
    "        {\"role\": \"assistant\", \"content\":response_template.replace(\"{TOPICCC}\", \"Apple File System (APFS)\")+previous_response},\n",
    "        {\"role\": \"user\", \"content\":prompt.replace(\"{TOPICCC}\", questions)},\n",
    "        {\"role\": \"assistant\", \"content\":response_template.replace(\"{TOPICCC}\", questions)}\n",
    "        ], tokenize=False)\n",
    "    formatted_prompt = formatted_prompt.removesuffix(\"</s>\")\n",
    "    return formatted_prompt\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE)):\n",
    "    batches = df[\"questions\"].iloc[i:i+BATCH_SIZE]\n",
    "    formatted_prompt =[generate_prompt(batch) for batch in batches]\n",
    "    results = llm.generate(formatted_prompt, sampling_params=sampling_params)\n",
    "    results_adj = [result.prompt.split(\"[/INST]\")[-1]+ result.outputs[0].text for result in results]\n",
    "    df.loc[batches.index, 'gen_questions'] = results_adj\n",
    "    df.to_parquet(\"results_questions.parquet\")   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnovi_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
