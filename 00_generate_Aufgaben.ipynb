{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generierung von Aufgaben zum Embedding: \n",
    "Wir verwenden den Quora scrape um verschiedene Topics zu erhalten. Damit generieren wir uns jeweils 20 diverse Aufgaben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# hf_hub_download(repo_id=\"SebastianBodza/Quora_deutsch_ger_Pairs_RL_DPO\", filename=\"output.jsonl\", repo_type=\"dataset\")\n",
    "import pandas as pd\n",
    "import random \n",
    "import urllib.parse\n",
    "\n",
    "quora = pd.read_json(\"/home/bodza/.cache/huggingface/hub/datasets--SebastianBodza--Quora_deutsch_ger_Pairs_RL_DPO/snapshots/f81129b4c2a5453b6c037e0571a52b701da8f6b8/output.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "def sample_topics(n=5): \n",
    "    samples = random.sample(quora[\"topic\"].unique().tolist(), n)\n",
    "    samples = [urllib.parse.unquote(encoded_string) for encoded_string in samples]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Create a list of potentially useful text retrieval tasks (RAG).\n",
    "Stick to the following guidelines:\n",
    "- Specify what the query is and what the requested documents are.\n",
    "- Each retrieval task should cover a wide range of requests and should not be too specific.\n",
    "\n",
    "Your output should always be just a list of strings, with about 5 elements each, and each element corresponds to a unique retrieval task in a set. Don't explain yourself or give anything else away. Be creative.\n",
    "\n",
    "Create these tasks in the following areas:\n",
    "{tasks}\n",
    "Create all entries completely in German! Never use English! Never use \"Sie\"! Never use the german word \"Sie\"!\"\"\"\n",
    "\n",
    "response_template =\"'{category_1}':\\n1.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Create a list of potentially useful text retrieval tasks (RAG).\n",
      "Stick to the following guidelines:\n",
      "- Specify what the query is and what the requested documents are.\n",
      "- Each retrieval task should cover a wide range of requests and should not be too specific.\n",
      "\n",
      "Your output should always be just a list of strings, with about 5 elements each, and each element corresponds to a unique retrieval task in a set. Don't explain yourself or give anything else away. Be creative.\n",
      "\n",
      "Create these tasks in the following areas:\n",
      "Eier-Nahrungsmittel\n",
      "Amerikanischer-Konservatismus\n",
      "Melbourne\n",
      "Quanten-Chromodynamik\n",
      "Selbst√§ndige-Unternehmer\n",
      "Create all entries completely in German! Never use English! Never use \"Sie\"! Never use the german word \"Sie\"! [/INST]'Eier-Nahrungsmittel':\n",
      "1.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch \n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "\n",
    "model_name = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "revision=\"gptq-4bit-128g-actorder_True\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model_q = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name,\n",
    "    revision=revision,\n",
    "    device=\"auto\",\n",
    "    use_triton=False,\n",
    "    inject_fused_attention=False,\n",
    "    inject_fused_mlp=False,\n",
    "    disable_exllama=True,\n",
    "    disable_exllamav2=True,\n",
    "    use_marlin=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for n in range(5000):\n",
    "    topics = sample_topics()\n",
    "    formatted_prompt = tokenizer.apply_chat_template(conversation=[{\"role\": \"user\", \"content\":prompt.format(tasks=\"\\n\".join(topics))},{\"role\": \"assistant\", \"content\":response_template.format(category_1=topics[0])}], tokenize=False)\n",
    "    formatted_prompt = formatted_prompt.removesuffix(\"</s>\")\n",
    "    print(formatted_prompt)\n",
    "\n",
    "    if n%10 == 0: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "model_name = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=revision)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "model_q = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_id,\n",
    "    device=\"cuda:0\",\n",
    "    use_triton=False,\n",
    "    inject_fused_attention=False,\n",
    "    inject_fused_mlp=False,\n",
    "    disable_exllama=disable_exllama,\n",
    "    disable_exllamav2=disable_exllamav2,\n",
    "    use_marlin=use_marlin,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnovi_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
