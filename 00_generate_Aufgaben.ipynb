{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generierung von Aufgaben zum Embedding: \n",
    "Wir verwenden den Quora scrape um verschiedene Topics zu erhalten. Damit generieren wir uns jeweils 20 diverse Aufgaben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# hf_hub_download(repo_id=\"SebastianBodza/Quora_deutsch_ger_Pairs_RL_DPO\", filename=\"output.jsonl\", repo_type=\"dataset\")\n",
    "import pandas as pd\n",
    "import random \n",
    "import urllib.parse\n",
    "\n",
    "#quora = pd.read_json(\"/home/bodza/.cache/huggingface/hub/datasets--SebastianBodza--Quora_deutsch_ger_Pairs_RL_DPO/snapshots/f81129b4c2a5453b6c037e0571a52b701da8f6b8/output.jsonl\", orient=\"records\", lines=True)\n",
    "quora = pd.read_csv('/home/bodza/.cache/huggingface/hub/datasets--SebastianBodza--Quora_Deutsch_ger/snapshots/b852cae090a09b06d443ccc8dcef36bc02ef74e2/Quora_deutsch.csv', index_col=0)\n",
    "quora = quora.drop_duplicates(subset=[\"topic\"])\n",
    "\n",
    "# def sample_topics(n=5): \n",
    "#     samples = random.sample(quora[\"topic\"].unique().tolist(), n)\n",
    "#     samples = [urllib.parse.unquote(encoded_string) for encoded_string in samples]\n",
    "#     return samples\n",
    "\n",
    "def sample_topics(n=3):\n",
    "    topics = sorted(quora[\"topic\"].unique().tolist())\n",
    "    for i in range(0, len(topics), n):\n",
    "        samples = topics[i:i+n]\n",
    "        samples = [urllib.parse.unquote(encoded_string) for encoded_string in samples]\n",
    "        yield samples\n",
    "\n",
    "topic_generator = sample_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Create a list of potentially useful text retrieval tasks (RAG).\n",
    "Stick to the following guidelines:\n",
    "- Specify what the query is and what the requested documents are.\n",
    "- Each retrieval task should cover a wide range of requests and should not be too specific.\n",
    "\n",
    "Your output should always be just a list of strings, with about 5 elements each, and each element corresponds to a unique retrieval task in a set. Don't explain yourself or give anything else away. Be creative.\n",
    "\n",
    "Create these tasks in the following areas:\n",
    "{tasks}\n",
    "and add five additional areas be creative! \n",
    "Create all entries completely in German! Never use English! Never use \"Sie\"! Never use the german word \"Sie\"!\"\"\"\n",
    "\n",
    "response_template =\"'{category_1}':\\n1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-23 15:54:57 config.py:457] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 01-23 15:54:57 config.py:175] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 15:54:58,876\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 15:54:59 llm_engine.py:70] Initializing an LLM engine with config: model='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer_mode=auto, revision=gptq-4bit-32g-actorder_True, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2000, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=gptq, enforce_eager=False, seed=0)\n",
      "INFO 01-23 15:55:19 llm_engine.py:275] # GPU blocks: 3564, # CPU blocks: 4096\n",
      "INFO 01-23 15:55:20 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-23 15:55:20 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "\u001b[36m(RayWorkerVllm pid=49045)\u001b[0m INFO 01-23 15:55:20 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=49045)\u001b[0m INFO 01-23 15:55:20 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=49045)\u001b[0m [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n",
      "[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 15:55:55 model_runner.py:547] Graph capturing finished in 35 secs.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import vllm \n",
    "import pandas as pd \n",
    "from vllm import SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "df = pd.DataFrame(columns=['topics', 'predicted_text'])\n",
    "sampling_params = SamplingParams(temperature=0.1, max_tokens=4000)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "llm = vllm.LLM(model=model_name, quantization=\"gptq\", dtype=torch.float16, tensor_parallel_size=2, max_model_len=2000, revision=\"gptq-4bit-32g-actorder_True\", gpu_memory_utilization=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [02:14<00:00,  4.21s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:32<00:00,  4.77s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:35<00:00,  4.87s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:45<00:00,  5.16s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:21<00:00,  4.44s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:28<00:00,  4.64s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:20<00:00,  4.40s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:27<00:00,  4.60s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:25<00:00,  4.56s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:20<00:00,  4.38s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [03:00<00:00,  5.65s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:38<00:00,  4.95s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:36<00:00,  4.90s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:30<00:00,  4.69s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:31<00:00,  4.75s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:45<00:00,  5.17s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:29<00:00,  4.67s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:38<00:00,  4.96s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:32<00:00,  4.78s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:38<00:00,  4.95s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:44<00:00,  5.13s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:24<00:00,  4.51s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:48<00:00,  5.28s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:35<00:00,  4.86s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:36<00:00,  4.90s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:42<00:00,  5.08s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:52<00:00,  5.40s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:29<00:00,  4.66s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:45<00:00,  5.16s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:38<00:00,  4.97s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:13<00:00,  4.17s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:37<00:00,  4.91s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:24<00:00,  4.50s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:38<00:00,  4.96s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:43<00:00,  5.11s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:32<00:00,  4.77s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:21<00:00,  4.41s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:50<00:00,  5.34s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:38<00:00,  4.97s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:47<00:00,  5.23s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:28<00:00,  4.64s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:32<00:00,  4.77s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:37<00:00,  4.93s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:28<00:00,  4.63s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:20<00:00,  4.39s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:43<00:00,  5.11s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:41<00:00,  5.06s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:43<00:00,  5.10s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:50<00:00,  5.34s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:29<00:00,  4.66s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:23<00:00,  4.49s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:37<00:00,  4.91s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:21<00:00,  4.42s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:42<00:00,  5.06s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:42<00:00,  5.08s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:33<00:00,  4.79s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:47<00:00,  5.25s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:46<00:00,  5.20s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:36<00:00,  4.89s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:33<00:00,  4.79s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:33<00:00,  4.80s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:42<00:00,  5.09s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:28<00:00,  4.64s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:46<00:00,  5.19s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:27<00:00,  4.60s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:38<00:00,  4.95s/it]\n",
      "Processed prompts: 100%|██████████| 32/32 [02:29<00:00,  4.67s/it]\n",
      " 43%|████▎     | 67/157 [2:53:30<3:53:04, 155.38s/it]\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5000\u001b[39m, BATCH_SIZE)):\n\u001b[0;32m---> 13\u001b[0m     topics, formatted_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[43m[\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     14\u001b[0m     results \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mgenerate(formatted_prompt, sampling_params\u001b[38;5;241m=\u001b[39msampling_params)\n\u001b[1;32m     15\u001b[0m     results_adj \u001b[38;5;241m=\u001b[39m [result\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m result\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5000\u001b[39m, BATCH_SIZE)):\n\u001b[0;32m---> 13\u001b[0m     topics, formatted_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(BATCH_SIZE)])\n\u001b[1;32m     14\u001b[0m     results \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mgenerate(formatted_prompt, sampling_params\u001b[38;5;241m=\u001b[39msampling_params)\n\u001b[1;32m     15\u001b[0m     results_adj \u001b[38;5;241m=\u001b[39m [result\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m result\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m, in \u001b[0;36mgenerate_prompt\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m():\n\u001b[0;32m----> 4\u001b[0m     topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(topic_generator)\n\u001b[1;32m      5\u001b[0m     formatted_prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(conversation\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:prompt\u001b[38;5;241m.\u001b[39mformat(tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(topics))},{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:response_template\u001b[38;5;241m.\u001b[39mformat(category_1\u001b[38;5;241m=\u001b[39mtopics[\u001b[38;5;241m0\u001b[39m])}], tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     formatted_prompt \u001b[38;5;241m=\u001b[39m formatted_prompt\u001b[38;5;241m.\u001b[39mremovesuffix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def generate_prompt():\n",
    "    topics = next(topic_generator)\n",
    "    formatted_prompt = tokenizer.apply_chat_template(conversation=[{\"role\": \"user\", \"content\":prompt.format(tasks=\"\\n\".join(topics))},{\"role\": \"assistant\", \"content\":response_template.format(category_1=topics[0])}], tokenize=False)\n",
    "    formatted_prompt = formatted_prompt.removesuffix(\"</s>\")\n",
    "    return (topics, formatted_prompt)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# df = pd.read_parquet(\"results.parquet\")\n",
    "df = pd.DataFrame(columns=[\"topics\", \"results\"])\n",
    "for n in tqdm(range(0, 5000, BATCH_SIZE)):\n",
    "    topics, formatted_prompt = zip(*[generate_prompt() for _ in range(BATCH_SIZE)])\n",
    "    results = llm.generate(formatted_prompt, sampling_params=sampling_params)\n",
    "    results_adj = [result.prompt.split(\"[/INST]\")[-1]+ result.outputs[0].text for result in results]\n",
    "    batch_df = pd.DataFrame({\"topics\": list(topics), \"results\": results_adj})\n",
    "    df = pd.concat([df, batch_df], ignore_index=True)\n",
    "    df.to_parquet(\"results.parquet\")   \n",
    "    # 2:30 min for 24 | \n",
    "    # Theoretisch for 32: 3:20 min \n",
    "    # Theoretisch 6:40 min mit 64 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnovi_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
