{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generierung von Aufgaben zum Embedding: \n",
    "Wir verwenden den Quora scrape um verschiedene Topics zu erhalten. Damit generieren wir uns jeweils 20 diverse Aufgaben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# hf_hub_download(repo_id=\"SebastianBodza/Quora_deutsch_ger_Pairs_RL_DPO\", filename=\"output.jsonl\", repo_type=\"dataset\")\n",
    "import pandas as pd\n",
    "import random \n",
    "import urllib.parse\n",
    "\n",
    "quora = pd.read_json(\"/home/bodza/.cache/huggingface/hub/datasets--SebastianBodza--Quora_deutsch_ger_Pairs_RL_DPO/snapshots/f81129b4c2a5453b6c037e0571a52b701da8f6b8/output.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "def sample_topics(n=5): \n",
    "    samples = random.sample(quora[\"topic\"].unique().tolist(), n)\n",
    "    samples = [urllib.parse.unquote(encoded_string) for encoded_string in samples]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Create a list of potentially useful text retrieval tasks (RAG).\n",
    "Stick to the following guidelines:\n",
    "- Specify what the query is and what the requested documents are.\n",
    "- Each retrieval task should cover a wide range of requests and should not be too specific.\n",
    "\n",
    "Your output should always be just a list of strings, with about 5 elements each, and each element corresponds to a unique retrieval task in a set. Don't explain yourself or give anything else away. Be creative.\n",
    "\n",
    "Create these tasks in the following areas:\n",
    "{tasks}\n",
    "Create all entries completely in German! Never use English! Never use \"Sie\"! Never use the german word \"Sie\"!\"\"\"\n",
    "\n",
    "response_template =\"'{category_1}':\\n1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-23 11:40:36 config.py:457] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 01-23 11:40:36 config.py:175] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 11:40:39,441\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 11:40:39 llm_engine.py:70] Initializing an LLM engine with config: model='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer_mode=auto, revision=gptq-4bit-32g-actorder_True, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2000, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=gptq, enforce_eager=False, seed=0)\n",
      "INFO 01-23 11:41:13 llm_engine.py:275] # GPU blocks: 3564, # CPU blocks: 4096\n",
      "INFO 01-23 11:41:14 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-23 11:41:14 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "\u001b[36m(RayWorkerVllm pid=10852)\u001b[0m INFO 01-23 11:41:14 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=10852)\u001b[0m INFO 01-23 11:41:14 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=10852)\u001b[0m [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n",
      "[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 11:41:49 model_runner.py:547] Graph capturing finished in 35 secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=10852)\u001b[0m INFO 01-23 11:41:49 model_runner.py:547] Graph capturing finished in 35 secs.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import vllm \n",
    "import pandas as pd \n",
    "from vllm import SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "df = pd.DataFrame(columns=['topics', 'predicted_text'])\n",
    "sampling_params = SamplingParams(temperature=0.1, max_tokens=4000)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "llm = vllm.LLM(model=model_name, quantization=\"gptq\", dtype=torch.float16, tensor_parallel_size=2, max_model_len=2000, revision=\"gptq-4bit-32g-actorder_True\", gpu_memory_utilization=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 24/24 [01:03<00:00,  2.66s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:24<00:00,  3.52s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:41<00:00,  4.22s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:18<00:00,  3.26s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:22<00:00,  3.46s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:29<00:00,  3.73s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:22<00:00,  3.45s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:27<00:00,  3.64s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:29<00:00,  3.74s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:35<00:00,  3.97s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:16<00:00,  3.20s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:49<00:00,  4.56s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:21<00:00,  3.40s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:35<00:00,  3.97s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:17<00:00,  3.21s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:29<00:00,  3.74s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:45<00:00,  4.38s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:46<00:00,  4.42s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:45<00:00,  4.38s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:26<00:00,  3.62s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:37<00:00,  4.06s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:38<00:00,  4.11s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:18<00:00,  3.29s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:33<00:00,  3.91s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:37<00:00,  4.06s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:49<00:00,  4.58s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:30<00:00,  3.75s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:21<00:00,  3.41s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:32<00:00,  3.86s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:29<00:00,  3.73s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:30<00:00,  3.78s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:47<00:00,  4.48s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:26<00:00,  3.60s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:40<00:00,  4.18s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:48<00:00,  4.53s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:30<00:00,  3.77s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:25<00:00,  3.57s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:20<00:00,  3.35s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:27<00:00,  3.63s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:39<00:00,  4.13s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:27<00:00,  3.65s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:39<00:00,  4.13s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:21<00:00,  3.39s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:22<00:00,  3.43s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:27<00:00,  3.63s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:52<00:00,  4.69s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:51<00:00,  4.64s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:37<00:00,  4.08s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:14<00:00,  3.12s/it]\n",
      "Processed prompts: 100%|██████████| 24/24 [01:30<00:00,  3.78s/it]\n",
      " 24%|██▍       | 50/209 [1:16:22<4:02:43, 91.59s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def generate_prompt():\n",
    "    topics = sample_topics()\n",
    "    formatted_prompt = tokenizer.apply_chat_template(conversation=[{\"role\": \"user\", \"content\":prompt.format(tasks=\"\\n\".join(topics))},{\"role\": \"assistant\", \"content\":response_template.format(category_1=topics[0])}], tokenize=False)\n",
    "    formatted_prompt = formatted_prompt.removesuffix(\"</s>\")\n",
    "    return (topics, formatted_prompt)\n",
    "\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "df = pd.DataFrame(columns=[\"topics\", \"results\"])\n",
    "for n in tqdm(range(0, 5000, BATCH_SIZE)):\n",
    "    topics, formatted_prompt = zip(*[generate_prompt() for _ in range(BATCH_SIZE)])\n",
    "    results = llm.generate(formatted_prompt, sampling_params=sampling_params)\n",
    "    results = [result.prompt.split(\"[/INST]\")[-1]+ result.outputs[0].text for result in results]\n",
    "    batch_df = pd.DataFrame({\"topics\": list(topics), \"results\": results})\n",
    "    df = pd.concat([df, batch_df], ignore_index=True)\n",
    "    df.to_parquet(\"results.parquet\")   \n",
    "    # raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnovi_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
